{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama 3.2 3B Instruct Model\n",
    "\n",
    "Using the XSum Dataset containing 37,500 documents and human-generated summaries (30,000 train, 3,750 test, 3,750 validation), the Large Language Model (LLM) will be fine-tuned on this dataset to improve its performance in text summarization tasks.\n",
    "\n",
    "The goal of the fine-tuning process is to improve the text summarization performance of the base Large Language Model (LLM) based on a set of evaluation metrics.\n",
    "\n",
    "The fine-tuning process will make use of Parameter-Efficient Fine-Tuning (PEFT), which will incorporate LoRa (Low-Rank Adaptation) to fine-tune a small number of model parameters instead of all of the model's parameters, allowing us to save computational and storage costs due to resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install unsloth\n",
    "# # Also get the latest nightly Unsloth!\n",
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070. Max memory: 11.994 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\unsloth\\models\\llama.py:1164: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"XXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:00<00:00, 133359.82 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:00<00:00, 85174.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [00:00<00:00, 96004.10 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id', 'text'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id', 'text'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id', 'text'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following text.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    # instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"document\"]\n",
    "    outputs      = examples[\"summary\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../datasets/xsum_dataset.hf\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:06<00:00, 4647.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainingArg = TrainingArguments(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    max_steps = -1,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs\",\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = None,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = trainingArg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070. Max memory = 11.994 GB.\n",
      "2.768 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 30,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 3,750\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n",
      "  0%|          | 1/3750 [00:18<19:04:38, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.6546, 'grad_norm': 1.4817416667938232, 'learning_rate': 4e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3750 [00:21<9:50:52,  9.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.8614, 'grad_norm': 1.9751536846160889, 'learning_rate': 8e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3750 [00:27<8:08:24,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.4919, 'grad_norm': 2.108494281768799, 'learning_rate': 0.00012, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/3750 [00:30<6:05:08,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.2709, 'grad_norm': 2.0180323123931885, 'learning_rate': 0.00016, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3750 [00:34<5:23:43,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7881, 'grad_norm': 1.8707059621810913, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3750 [00:39<5:14:31,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.1647, 'grad_norm': 1.5650972127914429, 'learning_rate': 0.00019994659546061417, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3750 [00:43<4:56:06,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.9987, 'grad_norm': 1.2413358688354492, 'learning_rate': 0.00019989319092122832, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3750 [00:47<4:52:27,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.8313, 'grad_norm': 1.202725887298584, 'learning_rate': 0.00019983978638184245, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3750 [00:51<4:38:06,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.3148, 'grad_norm': 1.311071753501892, 'learning_rate': 0.0001997863818424566, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3750 [00:56<4:43:24,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.5222, 'grad_norm': 1.432337760925293, 'learning_rate': 0.00019973297730307076, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3750 [00:59<4:13:06,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.4552, 'grad_norm': 1.789084553718567, 'learning_rate': 0.00019967957276368492, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/3750 [01:04<4:25:42,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7957, 'grad_norm': 2.4022693634033203, 'learning_rate': 0.00019962616822429908, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m<string>:385\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSummarize the following text.\\n\\n### Input:\\nGreig Laidlaw kicked three penalties and converted tries by Charlie Sharples and Richard Hibbard to put the Cherry and Whites 23-9 up at the break.Myler converted James Wilson\\'s try to reduce the margin to seven points.Media playback is not supported on this deviceDan Murphy then went in for the hosts, Wilson grabbed a second and Myler and Laidlaw traded penalties before Manoa crucially touched down.Northampton\\'s Premiership lead was cut to nine points, with Exeter Chiefs thrashing bottom side London Welsh 74-19 to go second.The draw saw Gloucester slip to ninth, 11 points adrift of fourth-placed Saracens, who face fellow play-off hopefuls Wasps on Sunday.Once-capped England fly-half Myler marked his 148th and club-record setting Premiership appearance with two kicks for the competition pacesetters early on.However, the lead was short lived with Laidlaw teeing up three penalties in response to put the hosts 9-6 up on 21 minutes.Both sides seemed to thrive in the sunny and unseasonably warm conditions at Kingsholm.James Hook orchestrated the game\\'s first try, beating three Saints players before angling an expertly weighted kick out to the right where Sharples collected to cross over. Laidlaw added the extras to make it 16-6.The try added even greater impetus to the Cherry and Whites\\' attack, with Sharples almost dashing in for a second before Hibbard was kept out by Alex Corbisiero.Hibbard\\'s wait to touchdown was short lived, with the hooker making the most of their one-man advantage following James Wilson\\'s yellow card, peeling off the back of a maul to crash over.Media playback is not supported on this deviceMyler slotted his third penalty with the last act of the first-half, then cut the deficit further by adding the extras after Wilson eased over for the first of two tries in six second-half minutes.A break from Steve McColl set the foundations for Gloucester\\'s third score, with Murphy stretching out to add to the home side\\'s advantage, only for some superb handling from of Manoa and wing Jamie Elliott sending Wilson over again soon after.Myler followed up his second conversation with his fourth penalty, having missed an earlier straight-forward attempt, to reduce the gap to four points with more than 22 minutes remaining.Gloucester thought they had sealed the win six minutes from time when substitute Henry Purdy crossed out wide, but the try was disallowed for an infringement during the build-up.Still Laidlaw edged the hosts further ahead, but it proved too little as Myler secured a share of the spoils following Manoa\\'s dramatic late effort.Gloucester director of rugby David Humphreys told BBC Radio Gloucestershire:\"I think it\\'ll not be for a couple of days when we get a chance to sit down and review it that we\\'ll be able to take a huge number of positives from that and from the way we played.\"We\\'re still searching for that 80-minute performance that will allow us to win games against the top teams.\"With a 23-9 lead, we\\'ve got to control the momentum and pace of the game, so maybe that was the time to slow it down.\"It\\'s really the first time we\\'ve been in that position and I\\'ve no doubt we\\'ll learn from that as a group.\"Northampton Saints director of rugby Jim Mallinder:\"We showed what a good side we are in that second half. We talked about getting our balance right, and we played some good rugby.\"We got two cracking tries out wide from James Wilson, then to be able to go to the maul at the death and get that score just shows we have got threats all over the field.\"You can\\'t come down to Kingsholm and play as we did in the first-half. They are a proud team and it is a proud rugby city, and we made some fundamental mistakes in terms of our game-management.\"Gloucester: McColl; Sharples, Meakes, Twelvetrees (capt), May; Hook, Laidlaw; Murphy, Hibbard, Afoa; Savage, Palmer, Kalamafoni, Kvesic, Evans.Replacements: Burns for McColl (71), Purdy for Sharples (50), Y. Thomas for Murphy (58), Dawidiuk for Hibbard (68), Moriarty for Kalamafoni (28), Stooke for G Evans (51). Not Used: Puafisi, Robson.Northampton Saints: Wilson; K Pisi, G Pisi, Stephenson, Elliott; Myler, Fotuali\\'i; Corbisiero, Haywood, Ma\\'afu, Lawes, Day, Wood (capt), Clark, Manoa.Replacements: Tuala for K Pisi (68), L Dickson for Fotuali\\'i (54), A Waller for Corbisiero (61), Denman for Ma\\'afu (50), S Dickinson for C Day (50), Dowson for Clark (65). Not Used: Williams, Oliver.Sin-bin: Wilson (28 mins)Referee: Greg GarnerAttendance: 16,000\\n\\n### Response:\\nNorthampton Saints suffered a frustrating defeat at Gloucester as they failed to take advantage of a yellow card for James Wilson.<|eot_id|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        dataset['train']['document'][0], # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/woshityj/llama_3.2_3B_Instruct_bnb_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"woshityj/llama_3.2_3B_Instruct_bnb_finetuned\", token = \"XXXXXXXXXXXXXXXXXXXX\") # Online saving\n",
    "tokenizer.push_to_hub(\"woshityj/llama_3.2_3B_Instruct_bnb_finetuned\", token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "The evaluation metrics that will be used to evaluate the text summarization performance of the Large Language Models (LLMs) are:\n",
    "1. METEOR (Metric for Evaluation of Translation with Explicit Ordering)\n",
    "2. ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "3. BERTScore\n",
    "4. BLEU (BiLingual Evaluation Understudy)\n",
    "5. G-Eval\n",
    "6. FactCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import evaluate\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"XXXXXXXXXXXXXXXXXXXX\"\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset = load_from_disk(\"../datasets/xsum_dataset.hf\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: woshityj/llama_3.2_3B_Instruct_bnb_finetuned\n",
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070. Max memory: 11.994 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "def load_peft_model():\n",
    "    \n",
    "    if (is_flash_attn_2_available() and (torch.cuda.get_device_capability(0)[0] >= 8)):\n",
    "        attn_implementation = \"flash_attention_2\"\n",
    "    else:\n",
    "        attn_implementation = \"sdpa\"\n",
    "    \n",
    "    print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "    model_id = \"woshityj/llama_3.2_3B_Instruct_bnb_finetuned\"\n",
    "    print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_id,\n",
    "        max_seq_length = 8192,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "        token = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "    )\n",
    "\n",
    "    peft_model.to(device)\n",
    "\n",
    "    return peft_model, tokenizer\n",
    "\n",
    "peft_model, tokenizer = load_peft_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['test']\n",
    "\n",
    "articles = dataset['document'][0:50]\n",
    "human_summaries = dataset['summary'][0:50]\n",
    "generated_summaries = []\n",
    "\n",
    "for idx, article in enumerate(articles):\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following text.\n",
    "\n",
    "### Input:\n",
    "{article}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    human_baseline_text_output = human_summaries[idx]\n",
    "    FastLanguageModel.for_inference(peft_model)\n",
    "    peft_model_output = peft_model.generate(**input_ids, max_new_tokens = 8192, temperature = 0.1)\n",
    "    prompt_length = input_ids['input_ids'].shape[1]\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_output[0][prompt_length:], skip_special_tokens = True)\n",
    "    generated_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_summaries, generated_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human Summary</th>\n",
       "      <th>Generated Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 22-year-old man has been charged with causin...</td>\n",
       "      <td>A 22-year-old man has been charged with causin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Shropshire charity has designated October 'B...</td>\n",
       "      <td>A charity is trying to dispel the myth that bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now that Hurricane Junior has blown through Wa...</td>\n",
       "      <td>The Trump Tower meeting between Donald Trump J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The new leader of Kensington and Chelsea Counc...</td>\n",
       "      <td>The new leader of Kensington and Chelsea Counc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The index of the UK's biggest 100 companies, t...</td>\n",
       "      <td>The FTSE 100 index has fallen by 4.67% in a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A \"river of filth\", a spate of gorse fires, an...</td>\n",
       "      <td>The papers this week are full of stories about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Four Welsh MPs are standing for election as ch...</td>\n",
       "      <td>Four MPs are vying to become the new chairs of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A French court has jailed 35 porters at the co...</td>\n",
       "      <td>A French auction house has been ordered to pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Investors must be quoted an \"all-in fee\" to ma...</td>\n",
       "      <td>The UK's financial regulator has announced a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>North Korean leader Kim Jong-il is paying his ...</td>\n",
       "      <td>North Korean leader Kim Jong-il has left his h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Women's FA Cup holders Arsenal Ladies thrashed...</td>\n",
       "      <td>Arsenal Ladies beat Chelsea Ladies 4-0 to reac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Three British acts have advanced to the finals...</td>\n",
       "      <td>Three Scottish comedians have made it through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>With just two weeks until America goes to the ...</td>\n",
       "      <td>It's been a big day in the US presidential rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Met Police have challenged a ruling that l...</td>\n",
       "      <td>The Metropolitan Police has lost its appeal ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>An 18-year-old man is due to appear in court c...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Arsenal need the spending power of a billionai...</td>\n",
       "      <td>Arsenal fans need a change of ownership to imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Greek people don't seem desperately gratef...</td>\n",
       "      <td>Greece's election victory for the left-wing Sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A council has agreed to spend Â£450,000 on a st...</td>\n",
       "      <td>A Â£100m plan to build a bypass around villages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Average wages for self-employed workers are lo...</td>\n",
       "      <td>The self-employed workforce in the UK has seen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A sickly baby goat being nursed back to health...</td>\n",
       "      <td>A three-month-old puppy has been living with h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Livingston have parted company with manager Ma...</td>\n",
       "      <td>Livingston have parted company with manager Ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A former care home in Norfolk which was due to...</td>\n",
       "      <td>A former care home is up for sale after a fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>England centre Manu Tuilagi is in line to retu...</td>\n",
       "      <td>Leicester Tigers centre Manu Tuilagi is \"getti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>London's key share index edged ahead on Thursd...</td>\n",
       "      <td>The FTSE 100 closed higher on Monday, with sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>North Korea says it has carried out an undergr...</td>\n",
       "      <td>North Korea has conducted its fifth nuclear te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Activity in the UK's industrial and constructi...</td>\n",
       "      <td>Industrial output fell in March, according to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Surgeons in London have carried out the first ...</td>\n",
       "      <td>A new way of preserving livers for transplanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A gang that smuggled Â£2m of cocaine and cannab...</td>\n",
       "      <td>A man who was shot and injured in a dissident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>When Native American Choctaw tribesman Waylon ...</td>\n",
       "      <td>The Choctaw Nation of Oklahoma has a long hist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Marco Fu completed an astonishing comeback as ...</td>\n",
       "      <td>Mark Selby beat Luca Brecel 13-11 to reach the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The Irish government has said there is no long...</td>\n",
       "      <td>The Irish government has confirmed that it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Australia stand-off Bernard Foley thinks count...</td>\n",
       "      <td>Scotland's Finn Russell is a \"major threat\" fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Two men have been convicted of murdering a fat...</td>\n",
       "      <td>A man has been found guilty of murdering his c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Talks are to be held in a bid to prevent furth...</td>\n",
       "      <td>A union has welcomed a move by Highlands and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Two Irish boxers have been investigated over b...</td>\n",
       "      <td>The Olympic Council of Ireland (OCI) has said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The mother and stepfather of a five-year-old b...</td>\n",
       "      <td>A woman and her partner have denied causing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Plans to overhaul learning support in Dumfries...</td>\n",
       "      <td>A plan to cut learning support posts in Dumfri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Tens of thousands of plants and flowers salvag...</td>\n",
       "      <td>A Chelsea flower show garden has been created ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The government has responded to a petition cal...</td>\n",
       "      <td>The government has said it will not comment on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Richard Hill has resigned as manager of Nation...</td>\n",
       "      <td>Eastleigh have sacked manager Gary Hill after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>In this series, we have heard how free banking...</td>\n",
       "      <td>The UK's banking charges are notoriously compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Do you know your Triassic from your Jurassic a...</td>\n",
       "      <td>Dinosaurs are the most popular prehistoric cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A man jailed for faking his own death in a can...</td>\n",
       "      <td>A man who faked his own death to claim life in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>On Friday the largest ever all-female expediti...</td>\n",
       "      <td>A group of women are about to embark on a jour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The parents of six students killed in a balcon...</td>\n",
       "      <td>The families of the six people who died in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>A council has threatened to take legal action ...</td>\n",
       "      <td>Hull City have been told to remove a new pitch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Aspiring Welsh dramatic soprano Mari Wyn Willi...</td>\n",
       "      <td>A Welsh soprano has been named as one of the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>The father of a 12-year-old boy swept into the...</td>\n",
       "      <td>A father has told of the moment he saved his s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Concerns about a king scallop \"fishing race\" i...</td>\n",
       "      <td>The Isle of Man government has introduced a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A relative of a Newcastle couple has been char...</td>\n",
       "      <td>A man has been charged with the murder of his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Human Summary  \\\n",
       "0   A 22-year-old man has been charged with causin...   \n",
       "1   A Shropshire charity has designated October 'B...   \n",
       "2   Now that Hurricane Junior has blown through Wa...   \n",
       "3   The new leader of Kensington and Chelsea Counc...   \n",
       "4   The index of the UK's biggest 100 companies, t...   \n",
       "5   A \"river of filth\", a spate of gorse fires, an...   \n",
       "6   Four Welsh MPs are standing for election as ch...   \n",
       "7   A French court has jailed 35 porters at the co...   \n",
       "8   Investors must be quoted an \"all-in fee\" to ma...   \n",
       "9   North Korean leader Kim Jong-il is paying his ...   \n",
       "10  Women's FA Cup holders Arsenal Ladies thrashed...   \n",
       "11  Three British acts have advanced to the finals...   \n",
       "12  With just two weeks until America goes to the ...   \n",
       "13  The Met Police have challenged a ruling that l...   \n",
       "14  An 18-year-old man is due to appear in court c...   \n",
       "15  Arsenal need the spending power of a billionai...   \n",
       "16  The Greek people don't seem desperately gratef...   \n",
       "17  A council has agreed to spend Â£450,000 on a st...   \n",
       "18  Average wages for self-employed workers are lo...   \n",
       "19  A sickly baby goat being nursed back to health...   \n",
       "20  Livingston have parted company with manager Ma...   \n",
       "21  A former care home in Norfolk which was due to...   \n",
       "22  England centre Manu Tuilagi is in line to retu...   \n",
       "23  London's key share index edged ahead on Thursd...   \n",
       "24  North Korea says it has carried out an undergr...   \n",
       "25  Activity in the UK's industrial and constructi...   \n",
       "26  Surgeons in London have carried out the first ...   \n",
       "27  A gang that smuggled Â£2m of cocaine and cannab...   \n",
       "28  When Native American Choctaw tribesman Waylon ...   \n",
       "29  Marco Fu completed an astonishing comeback as ...   \n",
       "30  The Irish government has said there is no long...   \n",
       "31  Australia stand-off Bernard Foley thinks count...   \n",
       "32  Two men have been convicted of murdering a fat...   \n",
       "33  Talks are to be held in a bid to prevent furth...   \n",
       "34  Two Irish boxers have been investigated over b...   \n",
       "35  The mother and stepfather of a five-year-old b...   \n",
       "36  Plans to overhaul learning support in Dumfries...   \n",
       "37  Tens of thousands of plants and flowers salvag...   \n",
       "38  The government has responded to a petition cal...   \n",
       "39  Richard Hill has resigned as manager of Nation...   \n",
       "40  In this series, we have heard how free banking...   \n",
       "41  Do you know your Triassic from your Jurassic a...   \n",
       "42  A man jailed for faking his own death in a can...   \n",
       "43  On Friday the largest ever all-female expediti...   \n",
       "44  The parents of six students killed in a balcon...   \n",
       "45  A council has threatened to take legal action ...   \n",
       "46  Aspiring Welsh dramatic soprano Mari Wyn Willi...   \n",
       "47  The father of a 12-year-old boy swept into the...   \n",
       "48  Concerns about a king scallop \"fishing race\" i...   \n",
       "49  A relative of a Newcastle couple has been char...   \n",
       "\n",
       "                                    Generated Summary  \n",
       "0   A 22-year-old man has been charged with causin...  \n",
       "1   A charity is trying to dispel the myth that bl...  \n",
       "2   The Trump Tower meeting between Donald Trump J...  \n",
       "3   The new leader of Kensington and Chelsea Counc...  \n",
       "4   The FTSE 100 index has fallen by 4.67% in a da...  \n",
       "5   The papers this week are full of stories about...  \n",
       "6   Four MPs are vying to become the new chairs of...  \n",
       "7   A French auction house has been ordered to pay...  \n",
       "8   The UK's financial regulator has announced a s...  \n",
       "9   North Korean leader Kim Jong-il has left his h...  \n",
       "10  Arsenal Ladies beat Chelsea Ladies 4-0 to reac...  \n",
       "11  Three Scottish comedians have made it through ...  \n",
       "12  It's been a big day in the US presidential rac...  \n",
       "13  The Metropolitan Police has lost its appeal ag...  \n",
       "14  A man has been arrested on suspicion of murder...  \n",
       "15  Arsenal fans need a change of ownership to imp...  \n",
       "16  Greece's election victory for the left-wing Sy...  \n",
       "17  A Â£100m plan to build a bypass around villages...  \n",
       "18  The self-employed workforce in the UK has seen...  \n",
       "19  A three-month-old puppy has been living with h...  \n",
       "20  Livingston have parted company with manager Ga...  \n",
       "21  A former care home is up for sale after a fire...  \n",
       "22  Leicester Tigers centre Manu Tuilagi is \"getti...  \n",
       "23  The FTSE 100 closed higher on Monday, with sha...  \n",
       "24  North Korea has conducted its fifth nuclear te...  \n",
       "25  Industrial output fell in March, according to ...  \n",
       "26  A new way of preserving livers for transplanta...  \n",
       "27  A man who was shot and injured in a dissident ...  \n",
       "28  The Choctaw Nation of Oklahoma has a long hist...  \n",
       "29  Mark Selby beat Luca Brecel 13-11 to reach the...  \n",
       "30  The Irish government has confirmed that it is ...  \n",
       "31  Scotland's Finn Russell is a \"major threat\" fo...  \n",
       "32  A man has been found guilty of murdering his c...  \n",
       "33  A union has welcomed a move by Highlands and I...  \n",
       "34  The Olympic Council of Ireland (OCI) has said ...  \n",
       "35  A woman and her partner have denied causing th...  \n",
       "36  A plan to cut learning support posts in Dumfri...  \n",
       "37  A Chelsea flower show garden has been created ...  \n",
       "38  The government has said it will not comment on...  \n",
       "39  Eastleigh have sacked manager Gary Hill after ...  \n",
       "40  The UK's banking charges are notoriously compl...  \n",
       "41  Dinosaurs are the most popular prehistoric cre...  \n",
       "42  A man who faked his own death to claim life in...  \n",
       "43  A group of women are about to embark on a jour...  \n",
       "44  The families of the six people who died in the...  \n",
       "45  Hull City have been told to remove a new pitch...  \n",
       "46  A Welsh soprano has been named as one of the t...  \n",
       "47  A father has told of the moment he saved his s...  \n",
       "48  The Isle of Man government has introduced a da...  \n",
       "49  A man has been charged with the murder of his ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(zipped_summaries, columns = ['Human Summary', 'Generated Summary'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR (Metric for Evaluation of Translation with Explicit Ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': 0.3190848229643983}\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "peft_model_meteor_results = meteor.compute(\n",
    "    predictions = generated_summaries,\n",
    "    references = human_summaries[0:len(generated_summaries)]\n",
    ")\n",
    "\n",
    "print(peft_model_meteor_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3820379985871919, 'rouge2': 0.16165955441775515, 'rougeL': 0.30418705127334356, 'rougeLsum': 0.3037050720068172}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "peft_model_rouge_results = rouge.compute(\n",
    "    predictions = generated_summaries,\n",
    "    references = human_summaries[0:len(generated_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True\n",
    ")\n",
    "\n",
    "print(peft_model_rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909839152097702\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "peft_model_bert_score_results = bert_score.compute(\n",
    "    predictions = df['Generated Summary'],\n",
    "    references = df['Human Summary'][0:len(df['Generated Summary'])],\n",
    "    lang = \"en\"\n",
    ")\n",
    "\n",
    "print(mean(peft_model_bert_score_results['precision']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU (BiLingual Evaluation Understudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.94k/5.94k [00:00<?, ?B/s]\n",
      "Downloading extra modules: 4.07kB [00:00, ?B/s]                       \n",
      "Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.34k/3.34k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.11133221754705928, 'precisions': [0.40307101727447214, 0.14818548387096775, 0.0881104033970276, 0.06053811659192825], 'brevity_penalty': 0.8333165886033752, 'length_ratio': 0.8457792207792207, 'translation_length': 1042, 'reference_length': 1232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bleu_score = evaluate.load(\"bleu\")\n",
    "\n",
    "peft_model_bleu_score_results = bleu_score.compute(\n",
    "    predictions = generated_summaries,\n",
    "    references = human_summaries[0:len(generated_summaries)]\n",
    ")\n",
    "\n",
    "print(peft_model_bleu_score_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/geneteated_summaries.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"./results/geneteated_summaries.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 3750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import evaluate\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset = load_from_disk(\"../datasets/xsum_dataset.hf\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./results/geneteated_summaries.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\accelerate\\utils\\modeling.py:841: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        )\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model = model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            use_cache = True,\n",
    "            device_map = \"auto\",\n",
    "            max_length = 8192,\n",
    "            do_sample = True,\n",
    "            top_k = 5,\n",
    "            num_return_sequences = 1,\n",
    "            eos_token_id = self.tokenizer.eos_token_id,\n",
    "            pad_token_id = self.tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        parser = JsonSchemaParser(schema.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(pipeline.tokenizer, parser)\n",
    "\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn = prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt) :]\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\"\n",
    "\n",
    "mistral_7b = CustomLlama3_8B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_compute_dtype = torch.float16,\n",
    "            bnb_4bit_quant_type = \"nf4\",\n",
    "            bnb_4bit_use_double_quant = True,\n",
    "        )\n",
    "\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        )\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model = model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            use_cache = True,\n",
    "            device_map = \"auto\",\n",
    "            max_length = 8192,\n",
    "            do_sample = True,\n",
    "            top_k = 5,\n",
    "            num_return_sequences = 1,\n",
    "            eos_token_id = self.tokenizer.eos_token_id,\n",
    "            pad_token_id = self.tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        parser = JsonSchemaParser(schema.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(pipeline.tokenizer, parser)\n",
    "\n",
    "        output_dict = pipeline(prompt, prefix_allowed_tokens_fn = prefix_function)\n",
    "        output = output_dict[0][\"generated_text\"][len(prompt) :]\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        return schema(**json_result)\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3.2 3B\"\n",
    "\n",
    "mistral_7b = Mistral7B()\n",
    "\n",
    "# mistral_7b = CustomLlama3_3B()\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"unsloth/mistral-7b-instruct-v0.1-bnb-4bit\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.1-bnb-4bit\")\n",
    "\n",
    "# mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)\n",
    "# print(mistral_7b.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Mistral patching. Transformers:4.47.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070. Max memory: 11.994 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\accelerate\\utils\\modeling.py:841: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import torch\n",
    "\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import (\n",
    "    build_transformers_prefix_allowed_tokens_fn,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class CustomMistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        model = self.load_model()\n",
    "\n",
    "        model_inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        parser = JsonSchemaParser(schema.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(self.tokenizer, parser)\n",
    "\n",
    "\n",
    "        model_inputs = model_inputs.to(\"cuda\")\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens = 8192, prefix_allowed_tokens_fn = prefix_function)\n",
    "        prompt_length = model_inputs['input_ids'].shape[1]\n",
    "        output = self.tokenizer.decode(generated_ids[0][prompt_length:])\n",
    "        json_result = json.loads(output)\n",
    "\n",
    "        return schema(**json_result) \n",
    "        # output = self.tokenizer.decode(generated_ids[0])\n",
    "        # print(output)\n",
    "        # json_result = json.loads(output)\n",
    "\n",
    "        # return schema(**json_result)\n",
    "    \n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.1-bnb-4bit\",\n",
    "    max_seq_length = 8192,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True)\n",
    "\n",
    "mistral_7b = CustomMistral7B(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Congratulations! Login successful ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Congratulations! Login successful ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"XXXXXXXXXXX\")\n",
    "\n",
    "OPENAI_API_KEY = \"XXXXXXXXXXXXXXXXX\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "coherence_metrics = GEval(\n",
    "    name = \"Coherence\",\n",
    "    # criteria = \"You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing and refer to it as needed.\",\n",
    "    evaluation_steps = [\n",
    "        \"Read the news article carefully and identify the main topic and key points.\",\n",
    "        \"Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.\",\n",
    "        \"Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the evaluation criteria.\"\n",
    "    ],\n",
    "    model = \"gpt-4o-mini\",\n",
    "    evaluation_params = [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 1 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 1 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/xsum_dataset.hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m test_case \u001b[38;5;241m=\u001b[39m LLMTestCase(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      8\u001b[0m     actual_output \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated Summary\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mcoherence_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\deepeval\\metrics\\g_eval\\g_eval.py:109\u001b[0m, in \u001b[0;36mGEval.measure\u001b[1;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_mode:\n\u001b[0;32m    108\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[1;32m--> 109\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_show_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_steps: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_evaluation_steps()\n\u001b[0;32m    115\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 2 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 2 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 34 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 34 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 3 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 3 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 4 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 4 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 35 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 35 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 5 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 5 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 36 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 36 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 37 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 37 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 38 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 38 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 8 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 8 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 39 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 39 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 9 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 9 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 40 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 40 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 10 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 10 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 41 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 41 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 11 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 11 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI rate limit exceeded. Retrying: 42 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI rate limit exceeded. Retrying: 42 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "dataset = load_from_disk(\"../datasets/xsum_dataset.hf\")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input = dataset['test'][0],\n",
    "    actual_output = df['Generated Summary'][0],\n",
    ")\n",
    "\n",
    "coherence_metrics.measure(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3404\\2854335272.py:30: PydanticDeprecatedSince20: The `schema` method \n",
       "is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 \n",
       "Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
       "  parser = JsonSchemaParser(schema.schema())\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3404\\2854335272.py:30: PydanticDeprecatedSince20: The `schema` method \n",
       "is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 \n",
       "Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
       "  parser = JsonSchemaParser(schema.schema())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 5 column 2 (char 162)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m test_case \u001b[38;5;241m=\u001b[39m LLMTestCase(\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], actual_output \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated Summary\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      7\u001b[0m metric \u001b[38;5;241m=\u001b[39m SummarizationMetric(\n\u001b[0;32m      8\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m      9\u001b[0m     model \u001b[38;5;241m=\u001b[39m mistral_7b,\n\u001b[0;32m     10\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(metric\u001b[38;5;241m.\u001b[39mscore)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(metric\u001b[38;5;241m.\u001b[39mreason)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\deepeval\\metrics\\summarization\\summarization.py:75\u001b[0m, in \u001b[0;36mSummarizationMetric.measure\u001b[1;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_mode:\n\u001b[0;32m     74\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_show_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruths: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_truths(test_case\u001b[38;5;241m.\u001b[39minput)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    232\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\deepeval\\metrics\\summarization\\summarization.py:127\u001b[0m, in \u001b[0;36mSummarizationMetric.a_measure\u001b[1;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    124\u001b[0m     async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator,\n\u001b[0;32m    126\u001b[0m ):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruths, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclaims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_truths(test_case\u001b[38;5;241m.\u001b[39minput),\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_claims(test_case\u001b[38;5;241m.\u001b[39mactual_output),\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m     (\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoverage_verdicts,\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malignment_verdicts,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_alignment_verdicts(),\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m     alignment_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_score(ScoreType\u001b[38;5;241m.\u001b[39mALIGNMENT)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Work\\Capstone\\venv\\lib\\site-packages\\deepeval\\metrics\\summarization\\summarization.py:501\u001b[0m, in \u001b[0;36mSummarizationMetric._a_generate_truths\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 501\u001b[0m         res: Truths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma_generate(prompt, schema\u001b[38;5;241m=\u001b[39mTruths)\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtruths\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mCustomMistral7B.a_generate\u001b[1;34m(self, prompt, schema)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ma_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, schema: BaseModel) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseModel:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m, in \u001b[0;36mCustomMistral7B.generate\u001b[1;34m(self, prompt, schema)\u001b[0m\n\u001b[0;32m     37\u001b[0m prompt_length \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m][prompt_length:])\n\u001b[1;32m---> 39\u001b[0m json_result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjson_result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 5 column 2 (char 162)"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import json\n",
    "\n",
    "test_case = LLMTestCase(input = dataset['test'][0], actual_output = df['Generated Summary'][0])\n",
    "metric = SummarizationMetric(\n",
    "    threshold = 0.5,\n",
    "    model = mistral_7b,\n",
    "    n = 5\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "evaluate([test_case], [metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
